数据处理脚本
========================================================

# 读入脚本 原始描述符数据为PBDEdes

```{r}
traindata <- read.table('traindata')
```
# 多元线性回归

采用stepwise去选择变量进行回归，使用leaps包进行变量选择

```{r MLR}
library(leaps)
# 最佳子集计算时间过长
# regfitfull <- regsubsets(RRT~.,ALLdata,really.big=T)
# 预测函数

predict.regsubsets = function(object,newdata,id,...) {
form = as.formula(object$call[[2]])
mat = model.matrix(form,newdata)
coefi = coef(object,id)
xvars = names (coefi)
mat[,xvars] %*% coefi
}

# 优化模型

set.seed(1)
train=sample(seq(180),150,replace=FALSE)
test <- traindata[-train,]
train <- traindata[train,]

k=5
set.seed (1)
folds = sample (1:k,nrow(train), replace = TRUE )
cv.errors = matrix (NA,k,50, dimnames = list (NULL,paste (1:50)))
for (j in 1:k) {
  best.fit = regsubsets (RRT~ ., data = train[folds!=j,],nvmax = 50,really.big=T,method="forward")
  for ( i in 1:50) {
    pred = predict.regsubsets(best.fit, train[folds == j,],id = i)
    cv.errors [j,i]= mean((train$RRT[folds == j] - pred)^2)
    }
  }

mean.cv.errors = apply(cv.errors,2,mean)

which.min (mean.cv.errors)
# 44
plot(mean.cv.errors, type='b')

reg.fwd = regsubsets(RRT~.,data = train,nvmax =50,really.big=T)

val.errors <- rep(NA ,45)
for ( i in 1:45) {
    pred = predict.regsubsets(reg.fwd, test,id = i)
    val.errors [i]= mean((test$RRT - pred)^2)
    }
plot(sqrt(val.errors),xlab="Number of Variables",ylab="Root MSE",ylim=c(0,0.04),pch=19,type="b")
points(sqrt(reg.fwd$rss[-1]/150),col="blue",pch=19,type="b")
legend("topright",legend=c("Training","Validation"),col=c("blue","black"),pch=19)
which.min (val.errors )
fwdrse <- mean(val.errors)

coef(reg.fwd,7)

#7

k=5
set.seed (1)
folds = sample (1:k,nrow(train), replace = TRUE )
cv.errors = matrix (NA,k,50, dimnames = list (NULL,paste (1:50)))
for (j in 1:k) {
best.fit = regsubsets (RRT~ ., data = train[folds!=j,],nvmax = 50,really.big=T,method="backward")
for ( i in 1:50) {
pred = predict.regsubsets(best.fit, train[folds == j,] , id = i )
cv.errors [j,i]= mean((train$RRT[folds == j] - pred)^2)
}
}

mean.cv.errors = apply(cv.errors,2,mean)

which.min (mean.cv.errors)
# 26
plot(mean.cv.errors, type='b')

reg.bwd = regsubsets(RRT~.,data = train,nvmax =50,really.big=T)

val.errors <- rep(NA ,45)
for(i in 1:45){
  pred = predict.regsubsets(reg.fwd, test,id = i)
    val.errors [i]= mean((test$RRT - pred)^2)
  }

png(file = 'RRTMLR.png',width = 800,height = 600)
plot(sqrt(val.errors),,xlab="Number of Variables",ylab="Root MSE",ylim=c(0,0.04),pch=19,type="b")
points(sqrt(reg.bwd$rss[-1]/150),col="blue",pch=19,type="b")
legend("topright",legend=c("Training","Validation"),col=c("blue","black"),pch=19)
dev.off()

which.min (val.errors )
bwdrse <- mean(val.errors)

coef(reg.bwd,7)

# 最终模型

coefi = coef(reg.bwd,7)
xvars = names (coefi)

form = as.formula(PBDEMDUNN)
mat = model.matrix(form,PBDEMDUNN)
RRTMLR <- mat[,xvars] %*% coefi

```

# 岭回归与lasso

```{r rlr}
library(glmnet)

set.seed(1)
train=sample(seq(180),150,replace=FALSE)

test <- traindata[-train,]

set.seed(1)
train=sample(seq(180),150,replace=FALSE)

train <- traindata[train,]

# ridge

fit.ridge=glmnet(model.matrix(RRT~.,train)[,-1],train[,1],alpha=0)
plot(fit.ridge,xvar="lambda",label=TRUE)
cv.ridge=cv.glmnet(model.matrix(RRT~.,train)[,-1],train[,1],alpha=0)
plot(cv.ridge)
bestlamr <- min(fit.ridge$lambda)
pred=predict(fit.ridge,model.matrix(RRT~.,test)[,-1],s=bestlamr)
ridgerse <- mean((pred - test[,1])^2)

# 最终模型

RRTridge=predict(fit.ridge,as.matrix(PBDEMDUN),s=bestlamr)

# lasso

fit.lasso=glmnet(model.matrix(RRT~.,train)[,-1],train[,1])
plot(fit.lasso,xvar="lambda",label=TRUE)
cv.lasso=cv.glmnet(model.matrix(RRT~.,train)[,-1],train[,1])
plot(cv.lasso)
bestlam =cv.lasso$lambda.min
pred=predict(fit.lasso,model.matrix(RRT~.,test)[,-1],s=bestlam)
lassorse <- mean((pred - test[,1])^2)
lasso.coef=predict (fit.lasso,type ='coefficients',s=bestlam )
lasso.coef[lasso.coef !=0]
# 41

# 最终模型

RRTlasso=predict(fit.lasso,as.matrix(PBDEMDUN),s=bestlam)

```

# 主成分回归与偏最小二乘回归

```{r ppr}

# PCR

library (pls)
set.seed (1)
pcr.fit=pcr(RRT~ .,data=train,validation ="CV")
summary (pcr.fit)
validationplot(pcr.fit ,val.type="MSEP",xlim=c(0,10))
pcr.pred=predict (pcr.fit,test)
pcrrsr <- rep(NA,134)
for(i in 1:134){
  pcrrse[i] <- mean((pcr.pred[,,i] - test$RRT)^2)
}
pcrrse <- mean((pcr.pred[,,134] - test$RRT)^2)

# 最终模型

RRTPCR=predict(pcr.fit,ncomp=134,PBDEMDUN)

# PLS

set.seed (1)
pls.fit=plsr(RRT~ .,data=train,validation ="CV")
summary (pls.fit)
validationplot(pls.fit ,xlim=c(0,120),ylim=c(0,0.02),val.type="MSEP")
pls.pred=predict (pls.fit,test)

plsrse <- mean((pls.pred[,,134] - test$RRT)^2)

# 最终模型

RRTPLS=predict(pls.fit,PBDEMDUN)

```

# SVM

```{r SVM}

library (e1071)
set.seed (1)
svmfit =svm(RRT~., train, kernel ="linear",cost=0.01)
tune.out = tune (svm,train[,-1],train$RRT, kernel="linear",ranges = list (cost = c (0.001 , 0.01 , 0.1 , 1 ,5 ,10)))
summary(tune.out)
svmfitradial =svm(RRT~., train, kernel ="radial",cost=5)
tune.out = tune (svm,train[,-1],train$RRT, kernel="radial",ranges = list (cost = c (0.001 , 0.01 , 0.1 , 1 ,5 ,10)))
summary(tune.out)
tune.out = tune (svm,train[,-1],train$RRT, kernel="polynomial",ranges = list (cost = c (0.001 , 0.01 , 0.1 , 1 ,5 ,10)))
summary(tune.out)
svmfitpolynomial =svm(RRT~., train, kernel ="polynomial",cost=1)
tune.out = tune (svm,train[,-1],train$RRT, kernel="sigmoid",ranges = list (cost = c (0.001 , 0.01 , 0.1 , 1 ,5 ,10)))
summary(tune.out)
svmfitsig =svm(RRT~., train, kernel ="sigmoid",cost=0.1)
# >0.01
ypred=predict(svmfit,test)
ypredradial=predict(svmfitradial,test)
ypredpoly=predict(svmfitpolynomial,test)
ypredsig=predict(svmfitsig,test)

ylres <- (ypred -test$RRT)^2
yrres <- (ypredradial -test$RRT)^2
ypres <- (ypredpoly-test$RRT)^2
ysres <- (ypredsig-test$RRT)^2
svmrse <- mean((ypred -test$RRT)^2)

# 最终模型

RRTSVM=predict(svmfit,PBDEMDUN)

```

# 树

```{r tree,cache=T}
library (tree)

# tree

set.seed (1)
tree.fit  <- tree(y[train]~x[train,])
cv.tree <- cv.tree(tree.fit )
plot(cv.tree$size,cv.tree$dev,type='b')
prune.tree <- prune.tree(tree.fit ,best = 7)
x0 <- data.frame(x)
yhat <- predict (tree.fit ,newdata =x0[-train ,])
trrse <- mean((yhat -y[-train])^2)

# RF

library (randomForest)

RF.fit <- randomForest(x=x[train,],y=y[train],xtest=x[-train,],ytest=y[-train],importance=T)
print(RF.fit)
RFrse <- mean((y[-train]-RF.fit$test[1]$predicted)^2)

# boosting

library (gbm)

set.seed (1)
bsrsei <- rep(NA,8)
index <- c(1,.5,.2,.1,.05,.02,.01,.005)
for(i in 1:8){
  Bs.fit <- gbm(RRT~.,ALLdata[train,-1],distribution="gaussian",n.trees=500,cv.folds=5,shrinkage=index[i])
  yhat <- predict (Bs.fit,newdata=ALLdata[-train ,-1],n.trees = 500)
  bsrsei[i] <- mean((y[-train]-yhat)^2)
}
bsrse <- min(bsrsei)
```

树方法低于其他回归方法。

# RSE对比

```{r RSE}
library(showtext)
png(file = 'dotchart.png',width = 800,height = 800)
RSE <- c(fwdrse,ridgerse,lassorse,pcrrse,plsrse,svmrse)
RSE
dotchart(RSE,cex=2,pch=19,labels=c('stepwise','ridge','lasso','pcr','pls','svm'),xlab='MSE')
dev.off()
```